{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "id": "EqqZChSMw_tW",
        "nbgrader": {
          "checksum": "f511eff26871492f3fa4ae077230fe51",
          "grade": false,
          "grade_id": "cell-f77284e87f75a760",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "source": [
        "# Health Information Systems and Decision Support Systems\n",
        "# WPO 5: - CAD Systems (15/03/2024)\n",
        "***\n",
        "*Panagiotis Gonidakis, Jakub Ceranka, Joris Wuts, Jef Vandemeulebrouke*<br>\n",
        "*Department of Electronics and Informatics (ETRO)*<br>\n",
        "*Vrije Universiteit Brussel, Pleinlaan 2, B-1050 Brussels, Belgium*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoznCU3xw_te"
      },
      "source": [
        "<font color=blue>Aline Jacquart - 0567936 <br>\n",
        "                 César Zapata - 0596811</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "id": "4HQWrOqFw_tf",
        "nbgrader": {
          "checksum": "35e56d1502506c7d95a2179be1975802",
          "grade": false,
          "grade_id": "cell-1180f9385da3b954",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "source": [
        "### Goal\n",
        "The goal of this practical session is to get an insight into artificial neural networks, more specifically multi-layer perceptrons (MLP) and convolutional neural networks (CNN). Your tasks will involve:\n",
        "- examining and pre-processing the real life lung nodule CT data\n",
        "- training various neural networks with increasing complexity to classify if there is a lung nodule on the CT image\n",
        "- validating the performance of your system against the manual ground-truth predictions prepared by an experienced radiologist.\n",
        "- visualizing training graphs and analysing information about model performance\n",
        "\n",
        "Students must send their notebook (and `wandb` generated report for all models in `pdf`) using the Assignment functionaly of Canvas before <b> 21/03/2024, 23:59 pm. </b>, in `.ipynb` and `.html` format. The grade from this practical session will contribute to your final grade.\n",
        "\n",
        "Questions: [jceranka@etrovub.be](mailto:jceranka@etrovub.be), [jwuts@etrovub.be](mailto:jwuts@etrovub.be)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "id": "dxhWsPacw_th",
        "nbgrader": {
          "checksum": "5287e4bf438bc9b077ef9f2e4bcec518",
          "grade": false,
          "grade_id": "cell-65dd4b67d3695b43",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "source": [
        "# Libraries\n",
        "During this practical session, the following libraries will be used:\n",
        "\n",
        "* Numpy (np)\n",
        "* Glob\n",
        "* Simple ITK (sitk)\n",
        "* Matplotlib\n",
        "* Sklearn\n",
        "* Tensorflow (tf)\n",
        "* Keras (you can also use pytorch if you are more familiar)\n",
        "* wandb\n",
        "\n",
        "To import any external library, you need to import it using the **import** statement followed by the name of the library and the shortcut. You can additionally check for the module version using **version** command.\n",
        "\n",
        "* If you use your own laptop, you will need to install new modules.\n",
        "\n",
        "* These expirements are simplified in order to be run without the need of a powerful GPU. However some training tasks may take 30-40 minutes using a CPU. You can accelerate your expirements if you work on [Google colab](https://colab.research.google.com/) framework where a GPU is offered. Then you need to create a GoogleDrive account and upload all the necessary data (scripts + data).\n",
        "For more information look [here](https://colab.research.google.com/) and [here](https://colab.research.google.com/notebooks/gpu.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "id": "PwCkQBzTw_ti",
        "nbgrader": {
          "checksum": "90dddf608397c1410372081b65393f39",
          "grade": false,
          "grade_id": "cell-cfeba9f5a5235a75",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "source": [
        "### Tensorflow and Keras\n",
        "\n",
        "TensorFlow (https://www.tensorflow.org/) is an end-to-end open-source platform for machine learning. It’s a comprehensive and flexible ecosystem of tools, libraries and other resources that provide workflows with high-level APIs. The framework offers various levels of concepts for you to choose the one you need to build and deploy machine learning models.\n",
        "\n",
        "Keras (https://keras.io/), on the other hand, is a high-level neural networks library that is running on the top of TensorFlow. Using Keras in deep learning allows for easy and fast prototyping as well as running seamlessly on CPU and GPU. This framework is written in Python code which is easy to debug and allows ease for extensibility. Keras offers simple and consistent high-level APIs and follows best practices to reduce the cognitive load for the users. Both frameworks provide high-level APIs for building and training models with ease. Keras is built in Python which makes it way more user-friendly than TensorFlow.\n",
        "\n",
        "In this exercise, <b>we recommend to use Keras</b> which allows to create/train/test neural networks with very few lines of understandable code.\n",
        "\n",
        "Keras requires Tensorflow to be installed. Normally, the new versions of Keras installs it automatically.\n",
        "\n",
        "To install Keras, open the anaconda prompt and then type:\n",
        " ```pip install keras```\n",
        "    \n",
        "If not done automatically:\n",
        " ```pip install tensorflow``` and after ```pip install keras```\n",
        "\n",
        "To start using visualization platform Weights-And-Biases go to (https://wandb.ai/) and create an account. Locally do: ```pip install wandb```\n",
        "\n",
        "\n",
        "Here are some useful resources (installation + documentation):\n",
        "\n",
        "* https://phoenixnap.com/kb/how-to-install-keras-on-linux <p>\n",
        "* https://www.tensorflow.org/install <p>\n",
        "* https://keras.io/getting_started/ <p>\n",
        "* https://keras.io/api/models/ <p>\n",
        "* https://keras.io/api/models/sequential/ <p>\n",
        "* https://docs.wandb.ai/quickstart <p>\n",
        "    \n",
        "(*) For the purpose of this exercise, you don't need to use a GPU; tensorflow and keras can be configured to run on a CPU and the most demanding training tasks should not take more than 30 min. On the other hand, [colab](https://colab.research.google.com/) offers GPU development environment and the training tasks can really be accelerated.\n",
        "\n",
        "(*) Make sure you are using tensorflow 2. It is advised to verify which version it is being used before looking for any documentation as the APIs differ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYbIbvRCz2AN",
        "outputId": "1b15f7bb-50d5-4eb5-d97b-42c4f3f16c72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KC_whuy70ZCm",
        "outputId": "05421cfa-def9-4d6f-f274-0056eb9ca1a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/UGent/Health-info-and-support/WPO-5\n",
            "\u001b[0m\u001b[01;34mimages\u001b[0m/            \u001b[01;34mscreenshots_tensorboard\u001b[0m/  train_data.npy\n",
            "\u001b[01;34m__MACOSX\u001b[0m/          test_data.npy             train_labels.npy\n",
            "\u001b[01;34mMiniLunaDataset3\u001b[0m/  test_labels.npy           WPO5_AlineJacquart_CesarZapata_2024.ipynb\n"
          ]
        }
      ],
      "source": [
        "# this is just the path in my drive for the WPO folder\n",
        "%cd drive/MyDrive/UGent/Health-info-and-support/WPO-5/\n",
        "%ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "id": "nKB_7Lagw_tk",
        "nbgrader": {
          "checksum": "8f6b761e553767615fad950fc7d54345",
          "grade": false,
          "grade_id": "cell-e04efc9902fe123b",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "source": [
        "# Lung Nodule Analysis\n",
        "Lung cancer is the leading cause of cancer-related deaths worldwide. Screening high risk individuals for lung cancer with low-dose CT scans is now being implemented in the United States and other countries are expected to follow soon. In CT lung cancer screening, millions of CT scans will have to be analyzed, which is an enormous burden for radiologists. Therefore, there is a lot of interest in development of computer algorithms to optimize cancer screening.\n",
        "\n",
        "<img src=\"images/luna16_image.png\">\n",
        "\n",
        "A vital first step in the analysis of lung cancer CT scans is the detection of pulmonary nodules, which may or may not represent early stage lung cancer. Many computer-aided detection (CAD) systems have already been proposed for this task.\n",
        "\n",
        "The LIDC/IDRI data set is publicly available, including the ground-truth annotations of nodules performed by four radiologists.\n",
        "\n",
        "This practical session is inspired from the challenge [LUNA16](https://luna16.grand-challenge.org/home/), which focused on a large-scale evaluation of automatic nodule detection algorithms on the LIDC/IDRI data set.\n",
        "\n",
        "<img src=\"images/lung_cancer1.png\">\n",
        "\n",
        "\n",
        "### Using LIDC/IDRI data set in this practical session\n",
        "\n",
        "For the needs of this practical session, we will look for an algorithm that only determines the likelihood for a given location in a CT scan to contain a pulmonary nodule. Furthermore, we have included a very small part of the LIDC/IDRI data set and we will use only a slice of suspicious regions of a CT scan.\n",
        "\n",
        "### Data augmentation\n",
        "\n",
        "Originally our dataset was very unbalanced. There were a lot samples of non-nodules (negatives) but very few samples of nodules (positives). Machine learning algorithms and specifically neural networks and convolutional neural networks require to be trained on balanced dataset, meaning all the classes should be equally represented in the training set.\n",
        "\n",
        "Using data augmentation methods (**rotation and translation**), positive samples were massively augmented in order to balance the two classes (nodules and non-nodules)\n",
        "\n",
        "### Ground Truth data\n",
        "\n",
        "Categorical data are variables that contain label values rather than numeric values. In our dataset, a sample can represent a nodule or a non-nodule area, so initially we have our ground truth data in a categorical form. Many machine learning algorithms cannot operate on label data directly. They require all input variables to be numeric. This means that categorical data must be converted to a numerical form. This involves two steps:\n",
        "1. Integer Encoding\n",
        "2. One-Hot Encoding.\n",
        "\n",
        "As a first step, each unique category value is assigned an integer value. That's why in our dataset, a sample which represents a nodule will have as a label `1` and a sample which represents a non-nodule area will have as a label `0`.\n",
        "\n",
        "For our case, this enconding step would be enough since we have only two categories. However, in a more general problem with more than two classes, using this encoding allows the model to assume a natural ordering between categories which may result in poor performance or unexpected results. That's why, one hot encoding can be appled to the integer representation. This is where the integer encoded variable is removed and a new binary variable is added for each unique integer value.\n",
        "\n",
        "Therefore, a  nodule will be represented by the binary variable `[1,0]` and a non-nodule area by the binary variable `[0,1]`.\n",
        "\n",
        "<img src=\"images/lung_cancer2.png\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "id": "kft0iSz8w_tm",
        "nbgrader": {
          "checksum": "77b5ce922c11056fda2cc0a605361876",
          "grade": false,
          "grade_id": "cell-88edf7d56c17540b",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "source": [
        "# Part 1: Visualize, load and prepare data for Machine Learning Algorithms\n",
        "\n",
        "CT images are stored in MetaImage (`mhd/raw`) format. Each .mhd file is stored with as a header file (`.mhd`) and a corresponding pixeldata file (`.raw`). To load a CT image, just **load the `.mhd` header file** and data from the binary .raw file will be automatically loaded.\n",
        "\n",
        "If you look carefully at the name of each .mhd file, you can extract useful information for a specific sample. You can identify its **number id**, its **size**, if the image was produced by a **data augmentation** method and if it contains a **nodule or not**.\n",
        "\n",
        "For example: *20046_x0y0z0_20x20x6_r0_1.mhd*\n",
        "* **20046**:   number of candidate patch\n",
        "* **x0y0z0**: no translation in any axis (if augmentation is used it is mentioned by the angle in the corresponding axis)\n",
        "* **20x20x6**: size of the image in voxels\n",
        "* **r0**:     no rotation\n",
        "* **1**:      it is a positive sample - represents a nodule\n",
        "* **.mhd**:   it is a mhd file (this is the file which can be loaded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "id": "iH-yaFFcw_tn",
        "nbgrader": {
          "checksum": "eaa451a946fc33d891799f2910eb06af",
          "grade": false,
          "grade_id": "cell-6e65384d14dac861",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "source": [
        "### Task 1: Import necessary libraries\n",
        "\n",
        "Load all necessary libraries using the **import** statement and check for errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfazfgCPACPA",
        "outputId": "f231b4db-11df-4ed7-9b62-287533110cd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting SimpleITK\n",
            "  Downloading SimpleITK-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: SimpleITK\n",
            "Successfully installed SimpleITK-2.3.1\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.16.4-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.42.0-py2.py3-none-any.whl (263 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.5/263.5 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.42 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.42.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.4\n"
          ]
        }
      ],
      "source": [
        "!pip install SimpleITK\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1LKweVwHw_to",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "3f854eb3-fe6b-472c-df58-237d72b30f15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Volume in drive C has no label.\n",
            " Volume Serial Number is 2F80-015A\n",
            "\n",
            " Directory of c:\\Users\\cesar\\Documents\\UGent\\2023-2024\\Second-term\\Health-Information-and-Decision-Support-Systems\\Health-Information\\WPO-5\n",
            "\n",
            "19/03/2024  17:07    <DIR>          .\n",
            "15/03/2024  09:02    <DIR>          ..\n",
            "15/03/2024  09:21    <DIR>          __MACOSX\n",
            "15/03/2024  09:21    <DIR>          images\n",
            "15/03/2024  09:19    <DIR>          MiniLunaDataset3\n",
            "15/03/2024  09:21    <DIR>          screenshots_tensorboard\n",
            "15/03/2024  09:19           742.528 test_data.npy\n",
            "15/03/2024  09:20             3.840 test_labels.npy\n",
            "15/03/2024  09:20         6.665.728 train_data.npy\n",
            "15/03/2024  09:20            33.456 train_labels.npy\n",
            "19/03/2024  17:07            60.211 WPO5_AlineJacquart_CesarZapata_2024.ipynb\n",
            "               5 File(s)      7.505.763 bytes\n",
            "               6 Dir(s)  276.705.865.728 bytes free\n"
          ]
        }
      ],
      "source": [
        "# Code for Part 1: Task 1\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "import keras\n",
        "import wandb\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import SimpleITK as sitk\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "%ls\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "id": "dihVp5kcw_tr",
        "nbgrader": {
          "checksum": "945c2ee4613c085addad9e466a2e2d03",
          "grade": false,
          "grade_id": "cell-1f6fe6d45fa0e86d",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "source": [
        "### Task 2: Inspect the dataset\n",
        "\n",
        "1. Using `SimpleITK`, create a function to load `.mhd` files (__Tip:__ The function should return a numpy array - `sitk.GetArrayFromImage( )`)\n",
        "2. Visualize the 6 slices of a chosen patch using `matplotlib` subplot figure and mention if it is a positive or a negative patch (Use colorscale `gray`)\n",
        "3. Visualize some augmented samples from the same candidate region using `matplotlib` subplot of the same patch and mention the augmentation method. Check visually if the observed patch was modified using the same augmentation method that is mentioned in the samples' filename.\n",
        "4. Count your files. How many positives and negatives there are in this dataset? (__Tip:__ Use `glob` library to get the number of specific files in your dataset)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0ATtfxRXK_n",
        "outputId": "cb260746-5016-4902-9a9f-968f6bcabc0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Volume in drive C has no label.\n",
            " Volume Serial Number is 2F80-015A\n",
            "\n",
            " Directory of c:\\Users\\cesar\\Documents\\UGent\\2023-2024\\Second-term\\Health-Information-and-Decision-Support-Systems\\Health-Information\\WPO-5\n",
            "\n",
            "19/03/2024  17:07    <DIR>          .\n",
            "15/03/2024  09:02    <DIR>          ..\n",
            "15/03/2024  09:21    <DIR>          __MACOSX\n",
            "15/03/2024  09:21    <DIR>          images\n",
            "15/03/2024  09:19    <DIR>          MiniLunaDataset3\n",
            "15/03/2024  09:21    <DIR>          screenshots_tensorboard\n",
            "15/03/2024  09:19           742.528 test_data.npy\n",
            "15/03/2024  09:20             3.840 test_labels.npy\n",
            "15/03/2024  09:20         6.665.728 train_data.npy\n",
            "15/03/2024  09:20            33.456 train_labels.npy\n",
            "19/03/2024  17:11            63.956 WPO5_AlineJacquart_CesarZapata_2024.ipynb\n",
            "               5 File(s)      7.509.508 bytes\n",
            "               6 Dir(s)  276.696.961.024 bytes free\n"
          ]
        }
      ],
      "source": [
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "IWtkfuzfTLkx",
        "outputId": "55e94463-c276-40dc-f5e5-c4d7724c85b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\cesar\\Documents\\UGent\\2023-2024\\Second-term\\Health-Information-and-Decision-Support-Systems\\Health-Information\\WPO-5\\20046_x-1y0z0_20x20x6_f0_1.mhd\n",
            "True\n",
            "MiniLunaDataset3\\20046_x-1y0z0_20x20x6_f0_1.mhd\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "Invalid shape (6, 20, 20) for image data",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[21], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m image \u001b[38;5;241m=\u001b[39m sitk\u001b[38;5;241m.\u001b[39mReadImage(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMiniLunaDataset3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m20046_x-1y0z0_20x20x6_f0_1.mhd\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m20\u001b[39m))\n\u001b[1;32m----> 8\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43msitk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGetArrayFromImage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGreys_r\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\matplotlib\\pyplot.py:3346\u001b[0m, in \u001b[0;36mimshow\u001b[1;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[0;32m   3325\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mimshow)\n\u001b[0;32m   3326\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimshow\u001b[39m(\n\u001b[0;32m   3327\u001b[0m     X: ArrayLike \u001b[38;5;241m|\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3344\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3345\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AxesImage:\n\u001b[1;32m-> 3346\u001b[0m     __ret \u001b[38;5;241m=\u001b[39m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3349\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3350\u001b[0m \u001b[43m        \u001b[49m\u001b[43maspect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maspect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3351\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3352\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3355\u001b[0m \u001b[43m        \u001b[49m\u001b[43morigin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3357\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation_stage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilternorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilternorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilterrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilterrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3360\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3361\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3362\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3363\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3364\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3365\u001b[0m     sci(__ret)\n\u001b[0;32m   3366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\matplotlib\\__init__.py:1478\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1475\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m   1476\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1478\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1480\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1481\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[0;32m   1482\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\matplotlib\\axes\\_axes.py:5751\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aspect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5749\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[1;32m-> 5751\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5752\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[0;32m   5753\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5754\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\matplotlib\\image.py:723\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[0;32m    722\u001b[0m     A \u001b[38;5;241m=\u001b[39m pil_to_array(A)  \u001b[38;5;66;03m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[1;32m--> 723\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_normalize_image_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_imcache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\matplotlib\\image.py:693\u001b[0m, in \u001b[0;36m_ImageBase._normalize_image_array\u001b[1;34m(A)\u001b[0m\n\u001b[0;32m    691\u001b[0m     A \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# If just (M, N, 1), assume scalar and apply colormap.\u001b[39;00m\n\u001b[0;32m    692\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m]):\n\u001b[1;32m--> 693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for image data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m    695\u001b[0m     \u001b[38;5;66;03m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[0;32m    696\u001b[0m     \u001b[38;5;66;03m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[0;32m    697\u001b[0m     \u001b[38;5;66;03m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[0;32m    698\u001b[0m     \u001b[38;5;66;03m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[0;32m    699\u001b[0m     high \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(A\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minteger) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
            "\u001b[1;31mTypeError\u001b[0m: Invalid shape (6, 20, 20) for image data"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x2000 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# path = \"drive/MyDrive/UGent/Health-info-and-support/WPO-5/\"\n",
        "# print(os.path.abspath('drive/MyDrive/UGent/Health-info-and-support/WPO-5/25637_x0y0z-1_20x20x6_q270_1.mhd'))\n",
        "print(os.path.abspath('20046_x-1y0z0_20x20x6_f0_1.mhd'))\n",
        "print(os.path.isfile(\"MiniLunaDataset3\\\\20046_x-1y0z0_20x20x6_f0_1.mhd\"))\n",
        "print(os.path.join('MiniLunaDataset3', '20046_x-1y0z0_20x20x6_f0_1.mhd'))\n",
        "image = sitk.ReadImage(os.path.join('MiniLunaDataset3', '20046_x-1y0z0_20x20x6_f0_1.mhd'))\n",
        "plt.figure(figsize=(5, 20))\n",
        "plt.imshow(sitk.GetArrayFromImage(image), cmap=plt.cm.Greys_r)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "collapsed": true,
        "id": "XNVQINzpw_ts",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "df93055d-fb1a-4ed1-b032-6e892ab803da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6\n",
            "['20046_x-1y0z0_20x20x6_f0_1.mhd', '20046_x-1y0z0_20x20x6_f180_1.mhd', '20046_x-1y0z0_20x20x6_f270_1.mhd', '20046_x-1y0z0_20x20x6_f90_1.mhd', '20046_x-1y0z0_20x20x6_q0_1.mhd', '20046_x-1y0z0_20x20x6_q180_1.mhd']\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "Invalid shape (2400,) for image data",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[18], line 51\u001b[0m\n\u001b[0;32m     47\u001b[0m     images_arr\u001b[38;5;241m.\u001b[39mappend(file)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(images_arr)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mimages_arr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 51\u001b[0m \u001b[43mshowImages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages_arr\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[18], line 17\u001b[0m, in \u001b[0;36mshowImages\u001b[1;34m(images)\u001b[0m\n\u001b[0;32m     14\u001b[0m figs, axs \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[0;32m     15\u001b[0m figs\u001b[38;5;241m.\u001b[39mtight_layout(pad\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3.0\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m \u001b[43maxs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr_images\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgrey\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m axs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimages[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m axs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mimshow(arr_images[\u001b[38;5;241m1\u001b[39m], cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrey\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\matplotlib\\__init__.py:1478\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1475\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m   1476\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1478\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1480\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1481\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[0;32m   1482\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\matplotlib\\axes\\_axes.py:5751\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aspect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5749\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[1;32m-> 5751\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5752\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[0;32m   5753\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5754\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\matplotlib\\image.py:723\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[0;32m    722\u001b[0m     A \u001b[38;5;241m=\u001b[39m pil_to_array(A)  \u001b[38;5;66;03m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[1;32m--> 723\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_normalize_image_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_imcache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\matplotlib\\image.py:693\u001b[0m, in \u001b[0;36m_ImageBase._normalize_image_array\u001b[1;34m(A)\u001b[0m\n\u001b[0;32m    691\u001b[0m     A \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# If just (M, N, 1), assume scalar and apply colormap.\u001b[39;00m\n\u001b[0;32m    692\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m]):\n\u001b[1;32m--> 693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for image data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m    695\u001b[0m     \u001b[38;5;66;03m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[0;32m    696\u001b[0m     \u001b[38;5;66;03m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[0;32m    697\u001b[0m     \u001b[38;5;66;03m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[0;32m    698\u001b[0m     \u001b[38;5;66;03m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[0;32m    699\u001b[0m     high \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(A\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minteger) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
            "\u001b[1;31mTypeError\u001b[0m: Invalid shape (2400,) for image data"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x800 with 6 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Code for Part 1: Task 2\n",
        "def loadImage(filename):\n",
        "    image = sitk.ReadImage(filename)\n",
        "    return image\n",
        "\n",
        "def showImages(images):\n",
        "    # plt.figure(figsize=(5, 20))\n",
        "    # plt.imshow(sitk.GetArrayViewFromImage(image), cmap=plt.cm.Greys_r)\n",
        "    arr_images = []\n",
        "    for image in images:\n",
        "      arr_images.append(loadImage(f\"MiniLunaDataset3/{image}\"))\n",
        "\n",
        "    # plotting images with corresponding histograms\n",
        "    figs, axs = plt.subplots(3, 2, figsize=(8, 8))\n",
        "    figs.tight_layout(pad=3.0)\n",
        "\n",
        "    axs[0][0].imshow(arr_images[0], cmap='grey')\n",
        "    axs[0][0].set_title(f\"{images[0]}\")\n",
        "\n",
        "    axs[0][1].imshow(arr_images[1], cmap='grey')\n",
        "    axs[0][1].set_title(f\"{images[1]}\")\n",
        "\n",
        "    axs[1][0].imshow(arr_images[2], cmap='grey')\n",
        "    axs[1][0].set_title(f\"{images[2]}\")\n",
        "\n",
        "    axs[1][1].imshow(arr_images[3], cmap='grey')\n",
        "    axs[1][1].set_title(f\"{images[3]}\")\n",
        "\n",
        "    axs[2][0].imshow(arr_images[4], cmap='grey')\n",
        "    axs[2][0].set_title(f\"{images[4]}\")\n",
        "\n",
        "    axs[2][1].imshow(arr_images[5], cmap='grey')\n",
        "    axs[2][1].set_title(f\"{images[5]}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# looking for the images in the patch: trying patch 25637\n",
        "dir_path = 'MiniLunaDataset3'\n",
        "files = os.listdir(dir_path)\n",
        "patch = '20046'\n",
        "\n",
        "images_arr = []\n",
        "# a ver como es la vuelta pues\n",
        "for file in files:\n",
        "  if len(images_arr) >= 6: break\n",
        "  if patch in file and 'mhd' in file:\n",
        "    images_arr.append(file)\n",
        "\n",
        "print(f\"{len(images_arr)}\\n{images_arr}\")\n",
        "\n",
        "showImages(images_arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "id": "1Sj1GX-Pw_tt",
        "nbgrader": {
          "checksum": "dcf46ab947dd55a873e1469c775a2f59",
          "grade": false,
          "grade_id": "cell-d9e7fbe181dfe944",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "source": [
        "### Task 3: Data pre-processing\n",
        "In order to use neural-networks, we need to pre-process the data and store it in a way which can be easily interpreted by keras.\n",
        "\n",
        "First of all, separate the test set from the train set. Keep 10% of your samples as a test-set. (**Tip:** Use `sklearn` `train_test_split` function.)\n",
        "\n",
        "Since 3D CNNs need a lot of computation power, we will convert our problem to 2D. Instead of using all the 6 CT slices, we will use only the 3rd CT slice. Therefore, from each sample, the 3rd slice wiil be extracted and further preprocessed. In addition, our values currently range from -1024 HU to around 2000 HU. Anything above 400 is not interesting to us, as these are simply bones with different radiodensity. A commonly used set of thresholds to normalize between is -1000 and 400. Finally the labels will be properly encoded to be used for training and testing.\n",
        "\n",
        "Write your pre-processing tasks as python functions (listed below) and in the end create a pipeline for each sample which will be also implemented as a function.\n",
        "1. Function loading the `.mhd` image as a numpy array.\n",
        "2. Function extracting the 3rd slice of a patch. The final form of patches should be a numpy array of size **20x20** pixels.\n",
        "3. Function normalizing the dataset.\n",
        "The unit of measurement in CT scans is the **Hounsfield Unit (HU)**, which is a measure of radiodensity. CT scanners are carefully calibrated to accurately measure this. From Wikipedia:\n",
        "<img src=\"images/HU_CTscannersCalibration.png\">\n",
        "\n",
        "    Create a function which is going to normalize the samples according to this table\n",
        "        * Create numpy arrays\n",
        "        * Normalize between [-1000, 400] using this normalization method\n",
        "    $npzarray = (npzarray - minHU) / (maxHU - minHU)$ <br>\n",
        "         where minHU = -1000 and maxHU = 400\n",
        "        * After this normalization set any values bigger than 1 to 1 and any values smaller than 0 to 0.\n",
        "\n",
        "4. Write an `If statement` encoding a label into a **1-hot** label as a ground-truth data in order to train the neural networks. (One-hot labels: `[0,1]` --> negative, `[1,0]` --> positive)\n",
        "\n",
        "5. Store into `.npy` data binary files. (__Tip:__ Use `np.save` to store data.)\n",
        "\n",
        "The pipeline should execute the functions as follows: <br>\n",
        "\n",
        "- Use `train_test_split` to divide the dataset into train and test subsets. Do that on patient level!\n",
        "- Create empty `np.arrays` filled with zeros to store patches and label data\n",
        "- Use for loop to iterate over all images in a train set:\n",
        "    - Load `.mhd` using `sitk`\n",
        "    - Normalize\n",
        "    - Extract 3rd slice\n",
        "    - Extract label as one-hot.\n",
        "    - Insert into empty array at correct index location\n",
        "- Save dataset.\n",
        "- Repeat for a test set.\n",
        "\n",
        "and return:\n",
        "* data:  [sample_idx,20,20]\n",
        "* labels: [sample_idx,2]\n",
        "\n",
        "Data pre-processing should be done for both: train and test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "hhsmnpy3w_tt",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Code for Part 1: Task 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "id": "soD08ptlw_tu",
        "nbgrader": {
          "checksum": "f0f3e11c93d2004ee46bad5992f6c714",
          "grade": false,
          "grade_id": "cell-f4edb7482a0753c4",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "source": [
        "In case you are not able to complete Part 1, you can move to Part 2 of the session by loading the provided `.npy` files. In that case you will **not** get any credits for Part 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "EC320UoMw_tu",
        "nbgrader": {
          "checksum": "687da3742ff425ca860c8478ed7d081a",
          "grade": false,
          "grade_id": "cell-d1fb33251b563bb8",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "source": [
        "# Part 2: Model training and validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "id": "hF_fpiLMw_tu",
        "nbgrader": {
          "checksum": "3d1de1e961e4781cd28635929dc63f5d",
          "grade": false,
          "grade_id": "cell-a11370ef4ea445ca",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "source": [
        "### Using Keras - A High Level API for Tensorflow\n",
        "\n",
        "Keras introduces a High-Level API that makes neural network building and training fast and easy. This API is intuitive and fully compatible with Tensorflow. In addition, keras provides a plethora of [APIs](https://keras.io/api/models/) and the user may select one that meets his needs. For the purpose of this exercise, the [sequential model](https://keras.io/api/models/sequential/) can be employed.\n",
        "\n",
        "### Visualization of training curves with `wandb`\n",
        "\n",
        "For the purpose of visualization in real-time the performance of our training, you will use Weights-And-Biases (`wandb`). During or after training you can use  it  to visualize the network and its performance. <b> For  all the following training tasks you are requested to include the loss and the accuracy graphs for training and also for evaluating the test-set</b>. A simple way to attach these graphs to jupyter notebook is to use generate a report after the training of all your networks is finished. Please upload that report in the pdf format and upload it in yur submission together with this notebook.\n",
        "\n",
        "Have a look at the following links for some tutorials on first use and report generation in `wandb`:\n",
        "* https://docs.wandb.ai/quickstart\n",
        "* https://docs.wandb.ai/guides/reports/create-a-report\n",
        "\n",
        "### Some Tips:\n",
        "\n",
        "* Use ```np.load``` to load your data from the previously created .npy binary files.\n",
        "* Use ```reshape``` method when it is needed to import data to the network.\n",
        "* Make your own function to create the model for each network architecture to avoid any model conflicts.\n",
        "\n",
        "* If you trained a model in a past python session, you can load the trained model but first you have to <b>define and initialize it again</b>.\n",
        "\n",
        "* Use `wandb` to visualize network and performance:\n",
        "\n",
        "* If you have strange keras errors, restart the python kernel and <b> re-run only the cells concerning your task you currently work (i.e. run the cells concerning only the current keras). Sometimes you cannot load two different keras at the same python session.</b>.\n",
        "\n",
        "(*) Depending the version of tensorflow you are working some commands might differ. Please look online for recent documentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "id": "_zZPPWoqw_tv",
        "nbgrader": {
          "checksum": "839c62166cf59b1aba0ebb12f7e38d6d",
          "grade": false,
          "grade_id": "cell-d5025d3539123bdb",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "source": [
        "### Task 1: Load Data from the numpy binary files (from Part 1)\n",
        "Use the `numpy` load function to load the data created in the previous part of the exercise. The data has to be reshaped in a specific way (use code below) in order to be compatible with the keras neural network input standard. After the data is successfully loaded, plot any patch and its ground-truth as image title to verify that data is correctly represented."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVLeES7ww_tv"
      },
      "outputs": [],
      "source": [
        "# Code for Part 2: Task 1\n",
        "\n",
        "X = train_data.reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
        "X_val = test_data.reshape(-1,IMG_SIZE,IMG_SIZE,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "id": "MxiFo4fEw_tv",
        "nbgrader": {
          "checksum": "0d1106dc6ff56598fb4c8afd41d9061f",
          "grade": false,
          "grade_id": "cell-0afd5980978d0695",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "source": [
        "### Task 2: Artificial Neural Networks (1 layer)\n",
        "First, lets train our classifier using an artificial neural network with just one layer (perceptron), a fully connected one (dense). This layer will have `softmax` as activation function and only two neurons.\n",
        "\n",
        "The main advantage of using this activation function is the output range. The range of the outputs will be `[0,1]` and the sum of all the probabilities will be equal to one. Generally, if the softmax function is used for a  multi-classification model it returns the probabilities of each class and the target class will have higher probability.\n",
        "\n",
        "Using Keras, define your model. Use these hyperparameters:\n",
        "* Learning Rate = `1e-4`\n",
        "* Batch Size = `8`\n",
        "* Optimizer = `sgd`\n",
        "* Loss metric = `categorical_crossentropy`\n",
        "* Validation metric score in Keras = `accuracy`\n",
        "\n",
        "Check your model architecture with `summary()` function.\n",
        "\n",
        "Since the network is small, it is not recommended to use `Dropout`. Train first for 1-2 epochs to verify that everything works and then train for ~10 epochs. To automatically integrate `wandb` in your Keras `model.fit` function, import `from wandb.keras import WandbMetricsLogger` and add a callback statement: `callbacks=[WandbMetricsLogger()]`. Observe the training and evaluate graphs from `wandb` to decide when to stop training.\n",
        "\n",
        "To evaluate the trained model, test it with the test samples (use `validation_data` in `model.fit` function).\n",
        "\n",
        "### Tip:\n",
        "* Use `.Flatten()` layer first to create a 1D vector in your input layer.\n",
        "* It is highly recommended to write all the tasks in functions and have as an input argument your keras. Then you will be able to re-use them in the next tasks for different keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "j56dXue3w_tw",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Code for Part 2: Task 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "id": "KCHB4e-tw_tw",
        "nbgrader": {
          "checksum": "729112bb739cb27832f2c27f1d3d3c5a",
          "grade": false,
          "grade_id": "cell-ec3cbbcde6634b2d",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "source": [
        "### Task 3: Test the model\n",
        "In this section you are going to write a <b>generic</b> validation function to assess the performance of out trained models. To do so, we will need the following modules from `sklearn`:\n",
        "- `confusion_matrix`\n",
        "- `accuracy_score`\n",
        "- `roc_curve`\n",
        "\n",
        "Import the modules and create a `python` function which takes two input arguments (validation test data and its GT labels). Use `model.predict` method to predict the class of our test set. Using the function print the `confusion_matrix`, `accuracy_score` and a `roc_curve` of this specific model.\n",
        "\n",
        "Use to function to assess the performance of your first train model. Print all of the validation metrics and a ROC curve.\n",
        "\n",
        "__Hint__: For the calculation of `confusion_matrix` and `accuracy_score` you need to transform your predicted and GT labels using `np.argmax`.\n",
        "\n",
        "__Hint__: For the calculation of `roc_curve` take only the second column of the predicted one-hot label e.g. `test_preds = test_preds[:,1]`\n",
        "\n",
        "Explain these graphs and the ROC curve. What role does the decision threshold play and why it is so important? Can we have a single value as the decision threshold? Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96ksnO_Sw_tw"
      },
      "outputs": [],
      "source": [
        "# Answer to the question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yh2FQbTuw_tx"
      },
      "outputs": [],
      "source": [
        "# Code for Part 2: Task 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "id": "fU6-4hAOw_tx",
        "nbgrader": {
          "checksum": "d82ad9587727760dd790c5846e9ba05f",
          "grade": false,
          "grade_id": "cell-ec462a6ad2168c48",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "source": [
        "### Task 4.1: Artificial Neural Networks (3 layers) - Let's go deep!\n",
        "\n",
        "From now on, we will try various networks with increasing complexity. Make sure with every network/parameter change, you update any hyperameters; you test and evaluate the new model with the previously used evaluation metrics (ROC curve, confusion matrix...). Moreover, every time check the training curves in `wandb`.\n",
        "\n",
        "Let's try 3 fully-connected layers using `sigmoid` activation function.\n",
        "\n",
        "Here is an example network but you are free to choose your own network.\n",
        "* 1st dense layer with 80 neurons, `sigmoid` activation\n",
        "* 2nd dense layer with 40 neurons, `sigmoid` activation\n",
        "* 3rd dense layer with 2 neurons and `softmax` activation\n",
        "* optimizer: `sgd`\n",
        "* learning rate: `1e-4`\n",
        "\n",
        "Explain your network's architecture and **test** your system as you did for the previous simple neural network. What do you observe? Make sure you tune again any hyperparameters like the amount of training epochs. Choose different values for the learning rate, number of epochs and batch size (e.g. 10, 20, 30...). Can you find some training curves which indicate **underfitting** and **overfitting**? If so, what network parameters play a crucial role in observing **underfitting** or **overfitting**?\n",
        "\n",
        "**Remark:** Some deeper models require more training to reach their optimal performance. If possible with your computational resources, try to train them for more epochs until no increase in accuracy is observed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aa1LItpRw_ty"
      },
      "outputs": [],
      "source": [
        "# Answer to the question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "s24svd9Rw_ty",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Code for Part 2: Task 4.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "X9jkrp-_w_tz",
        "nbgrader": {
          "checksum": "822f2a649d5f2f9cebf5cfb29e5681f1",
          "grade": false,
          "grade_id": "cell-2f4b91a56afdcffa",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "source": [
        "### Task 4.2: Relu!\n",
        "RELU = Rectified Linear Unit\n",
        "\n",
        "Let's try the same network but using RELU instead of sigmoid now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_yEak5w5w_tz",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Code for Part 2: Task 4.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "id": "Q-dC3UrBw_t0",
        "nbgrader": {
          "checksum": "9daeec5dcb3d381740aeac3613a7b8ba",
          "grade": false,
          "grade_id": "cell-a6394cf303cb2289",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "source": [
        "### Task 4.3: Dropout!\n",
        "Now lets add two dropout layers (each after the dense layer with relu activation). Some recommended values to investigate are 0.9, 0.5, 0.1. Explain the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLp1S7Vbw_t0"
      },
      "outputs": [],
      "source": [
        "# Your answer here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cbyNUbVjw_t1",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Code for Part 2: Task 4.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "id": "nlGssC0Fw_t1",
        "nbgrader": {
          "checksum": "c7c0f2e845aafc10ec8ad052d48e9119",
          "grade": false,
          "grade_id": "cell-2df072bbe1906a59",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "source": [
        "### Task 5. A Convolutional Neural Network\n",
        "\n",
        "Finally, we are going to implement a convolutional neural network - a network type showing state-of-the-art results in learning from images. This time, your input will not be flattened but we will provide a 2D image as an input (20x20x1 pixels). Use the same hyperparameters as before, unless specified differenly.\n",
        "\n",
        "#### Model definition\n",
        "\n",
        "input -> conv1 -> pool1 -> conv2 -> dense1 -> dense2 -> softmax\n",
        "* convolution layers: kernel size = 3x3, number of channels = 64, activation function = sigmoid\n",
        "* max-pooling layers: kernel size = 2x2, strides = 2\n",
        "* 1st dense layer: 50 neurons, activation function = sigmoid\n",
        "* 2nd dense layer: 2 neurons, activation function = softmax\n",
        "* optimizer = `sgd`\n",
        "* use dropout (e.g. 0.2)\n",
        "\n",
        "__Hint__: Use `.Flatten()` after before the dense layers to ensure a 1D vector as an input.\n",
        "\n",
        "Like before, measure system's prerfomance and explain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "oSYI-fU9w_t1",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Code for Part 2: Task 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "id": "lZ2ai6p_w_t2",
        "nbgrader": {
          "checksum": "c7c0f2e845aafc10ec8ad052d48e9119",
          "grade": false,
          "grade_id": "cell-2df072bbe1906a59",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "source": [
        "### Task 6. A Deeper Convolutional Neural Network\n",
        "\n",
        "#### Model definition\n",
        "\n",
        "input -> conv1 -> pool1 -> conv2 -> conv3 -> fc1 -> fc2 -> softmax\n",
        "* convolution layers: kernel size = 5x5, number of channels = 64, activation function = 'relu', padding = 'same'\n",
        "* max-pooling layers: kernel size = 2x2, strides = 2\n",
        "* 1st fully connected: 100 neurons, activation function = relu\n",
        "* 2nd fully connected: 2 neurons, activation function = softmax\n",
        "* optimizer = `sgd` (try also `adam`)\n",
        "* use dropout (e.g. 0.5)\n",
        "\n",
        "Like before, measure system's prerfomance and explain.\n",
        "\n",
        "This was the last model you have trained. The `wandb` should now allow you to generate a common report for all train models (on the same graphs). Generate the report, download it as a `pdf`, and upload it in your assignment together with the notebook file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Wt6z7LkBw_t2",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Code for Part 2: Task 6"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "celltoolbar": "Edit Metadata",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
